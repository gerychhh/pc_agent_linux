# --- PC Agent local settings (.env) ---
# LLM endpoint (llama-server OpenAI-compatible)
PC_AGENT_BASE_URL=http://127.0.0.1:1234/v1
PC_AGENT_MODEL_NAME=qwen2.5-7b

# Autostart llama.cpp server from app.py (1=on)
PC_AGENT_AUTOSTART_LLAMACPP=1

# Allow running sudo in autogenerated scripts (dangerous!)
# 0 = block sudo, 1 = allow (still blocks mkfs/parted/fdisk/dd/rm -rf)
PC_AGENT_ALLOW_SUDO=0

# llama-server binary + model (GGUF)
LLAMA_SERVER_BIN=third_party/llama.cpp/build/bin/llama-server
LLAMA_SERVER_MODEL=models/Qwen2.5-7B-Instruct-Q4_K_M.gguf

# Server performance (6GB VRAM laptop: start modest; auto-fit will reduce if OOM)
LLAMA_SERVER_PORT=1234
LLAMA_SERVER_CTX=2048
LLAMA_SERVER_N_GPU_LAYERS=20
LLAMA_SERVER_THREADS=8

# Auto-fit retries when OOM (reduce layers by STEP each retry)
LLAMA_SERVER_AUTOFIT_RETRIES=6
LLAMA_SERVER_AUTOFIT_STEP=5
